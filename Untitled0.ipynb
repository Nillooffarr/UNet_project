{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ymTW-8tcKo3petnbluMt3AYKrvsTtt-U",
      "authorship_tag": "ABX9TyMkkJkJJ1ksS3z9+Fm3twG9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nillooffarr/UNet_project/blob/Version1/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j52hsL6DMa5h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "outputId": "48def825-de54-4fef-a142-8442705e5030"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np,sys,os\n",
        "from sklearn.utils import shuffle\n",
        "from scipy.ndimage import imread\n",
        "from scipy.misc import imresize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(678)\n",
        "tf.set_random_seed(5678)\n",
        "\n",
        "def tf_relu(x): return tf.nn.relu(x)\n",
        "def d_tf_relu(s): return tf.cast(tf.greater(s,0),dtype=tf.float32)\n",
        "def tf_softmax(x): return tf.nn.softmax(x)\n",
        "def np_sigmoid(x): 1/(1 + np.exp(-1 *x))\n",
        "\n",
        "# --- make class ---\n",
        "class conlayer_left():\n",
        "    \n",
        "    def __init__(self,ker,in_c,out_c):\n",
        "        self.w = tf.Variable(tf.random_normal([ker,ker,in_c,out_c],stddev=0.05))\n",
        "\n",
        "    def feedforward(self,input,stride=1,dilate=1):\n",
        "        self.input  = input\n",
        "        self.layer  = tf.nn.conv2d(input,self.w,strides = [1,stride,stride,1],padding='SAME')\n",
        "        self.layerA = tf_relu(self.layer)\n",
        "        return self.layerA\n",
        "\n",
        "class conlayer_right():\n",
        "    \n",
        "    def __init__(self,ker,in_c,out_c):\n",
        "        self.w = tf.Variable(tf.random_normal([ker,ker,in_c,out_c],stddev=0.05))\n",
        "\n",
        "    def feedforward(self,input,stride=1,dilate=1,output=1):\n",
        "        self.input  = input\n",
        "\n",
        "        current_shape_size = input.shape\n",
        "\n",
        "        self.layer = tf.nn.conv2d_transpose(input,self.w,\n",
        "        output_shape=[batch_size] + [int(current_shape_size[1].value*2),int(current_shape_size[2].value*2),int(current_shape_size[3].value/2)],strides=[1,2,2,1],padding='SAME')\n",
        "        self.layerA = tf_relu(self.layer)\n",
        "        return self.layerA\n",
        "\n",
        "# --- get data ---\n",
        "data_location = \"/content/drive/My Drive/Colab Notebooks/training/images\"\n",
        "train_data = []  # create an empty list\n",
        "for dirName, subdirList, fileList in sorted(os.walk(data_location)):\n",
        "    for filename in fileList:\n",
        "        if \".tif\" in filename.lower():  # check whether the file's DICOM\n",
        "            train_data.append(os.path.join(dirName,filename))\n",
        "\n",
        "data_location = \"/content/drive/My Drive/Colab Notebooks/training/mask\"\n",
        "train_data_gt = []  # create an empty list\n",
        "for dirName, subdirList, fileList in sorted(os.walk(data_location)):\n",
        "    for filename in fileList:\n",
        "        if \".tif\" in filename.lower():  # check whether the file's DICOM\n",
        "            train_data_gt.append(os.path.join(dirName,filename))\n",
        "\n",
        "\n",
        "train_images = np.zeros(shape=(128,256,256,1))\n",
        "train_labels = np.zeros(shape=(128,256,256,1))\n",
        "\n",
        "for file_index in range(len(train_data)):\n",
        "    train_images[file_index,:,:]   = np.expand_dims(imresize(imread(train_data[file_index],mode='F',flatten=True),(256,256)),axis=2)\n",
        "    train_labels[file_index,:,:]   = np.expand_dims(imresize(imread(train_data_gt[file_index],mode='F',flatten=True),(256,256)),axis=2)\n",
        "\n",
        "train_images = (train_images - train_images.min()) / (train_images.max() - train_images.min())\n",
        "train_labels = (train_labels - train_labels.min()) / (train_labels.max() - train_labels.min())\n",
        "\n",
        "# --- hyper ---\n",
        "num_epoch = 100\n",
        "init_lr = 0.0001\n",
        "batch_size = 2\n",
        "\n",
        "# --- make layer ---\n",
        "# left\n",
        "l1_1 = conlayer_left(3,1,3)\n",
        "l1_2 = conlayer_left(3,3,3)\n",
        "l1_3 = conlayer_left(3,3,3)\n",
        "\n",
        "l2_1 = conlayer_left(3,3,6)\n",
        "l2_2 = conlayer_left(3,6,6)\n",
        "l2_3 = conlayer_left(3,6,6)\n",
        "\n",
        "l3_1 = conlayer_left(3,6,12)\n",
        "l3_2 = conlayer_left(3,12,12)\n",
        "l3_3 = conlayer_left(3,12,12)\n",
        "\n",
        "l4_1 = conlayer_left(3,12,24)\n",
        "l4_2 = conlayer_left(3,24,24)\n",
        "l4_3 = conlayer_left(3,24,24)\n",
        "\n",
        "l5_1 = conlayer_left(3,24,48)\n",
        "l5_2 = conlayer_left(3,48,48)\n",
        "l5_3 = conlayer_left(3,48,24)\n",
        "\n",
        "# right\n",
        "l6_1 = conlayer_right(3,24,48)\n",
        "l6_2 = conlayer_left(3,24,24)\n",
        "l6_3 = conlayer_left(3,24,12)\n",
        "\n",
        "l7_1 = conlayer_right(3,12,24)\n",
        "l7_2 = conlayer_left(3,12,12)\n",
        "l7_3 = conlayer_left(3,12,6)\n",
        "\n",
        "l8_1 = conlayer_right(3,6,12)\n",
        "l8_2 = conlayer_left(3,6,6)\n",
        "l8_3 = conlayer_left(3,6,3)\n",
        "\n",
        "l9_1 = conlayer_right(3,3,6)\n",
        "l9_2 = conlayer_left(3,3,3)\n",
        "l9_3 = conlayer_left(3,3,3)\n",
        "\n",
        "l10_final = conlayer_left(3,3,1)\n",
        "\n",
        "# ---- make graph ----\n",
        "x = tf.placeholder(shape=[None,256,256,1],dtype=tf.float32)\n",
        "y = tf.placeholder(shape=[None,256,256,1],dtype=tf.float32)\n",
        "\n",
        "layer1_1 = l1_1.feedforward(x)\n",
        "layer1_2 = l1_2.feedforward(layer1_1)\n",
        "layer1_3 = l1_3.feedforward(layer1_2)\n",
        "\n",
        "layer2_Input = tf.nn.max_pool(layer1_3,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
        "layer2_1 = l2_1.feedforward(layer2_Input)\n",
        "layer2_2 = l2_2.feedforward(layer2_1)\n",
        "layer2_3 = l2_3.feedforward(layer2_2)\n",
        "\n",
        "layer3_Input = tf.nn.max_pool(layer2_3,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
        "layer3_1 = l3_1.feedforward(layer3_Input)\n",
        "layer3_2 = l3_2.feedforward(layer3_1)\n",
        "layer3_3 = l3_3.feedforward(layer3_2)\n",
        "\n",
        "layer4_Input = tf.nn.max_pool(layer3_3,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
        "layer4_1 = l4_1.feedforward(layer4_Input)\n",
        "layer4_2 = l4_2.feedforward(layer4_1)\n",
        "layer4_3 = l4_3.feedforward(layer4_2)\n",
        "\n",
        "layer5_Input = tf.nn.max_pool(layer4_3,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
        "layer5_1 = l5_1.feedforward(layer5_Input)\n",
        "layer5_2 = l5_2.feedforward(layer5_1)\n",
        "layer5_3 = l5_3.feedforward(layer5_2)\n",
        "\n",
        "layer6_Input = tf.concat([layer5_3,layer5_Input],axis=3)\n",
        "layer6_1 = l6_1.feedforward(layer6_Input)\n",
        "layer6_2 = l6_2.feedforward(layer6_1)\n",
        "layer6_3 = l6_3.feedforward(layer6_2)\n",
        "\n",
        "layer7_Input = tf.concat([layer6_3,layer4_Input],axis=3)\n",
        "layer7_1 = l7_1.feedforward(layer7_Input)\n",
        "layer7_2 = l7_2.feedforward(layer7_1)\n",
        "layer7_3 = l7_3.feedforward(layer7_2)\n",
        "\n",
        "layer8_Input = tf.concat([layer7_3,layer3_Input],axis=3)\n",
        "layer8_1 = l8_1.feedforward(layer8_Input)\n",
        "layer8_2 = l8_2.feedforward(layer8_1)\n",
        "layer8_3 = l8_3.feedforward(layer8_2)\n",
        "\n",
        "layer9_Input = tf.concat([layer8_3,layer2_Input],axis=3)\n",
        "layer9_1 = l9_1.feedforward(layer9_Input)\n",
        "layer9_2 = l9_2.feedforward(layer9_1)\n",
        "layer9_3 = l9_3.feedforward(layer9_2)\n",
        "\n",
        "layer10 = l10_final.feedforward(layer9_3)\n",
        "\n",
        "cost = tf.reduce_mean(tf.square(layer10-y))\n",
        "auto_train = tf.train.AdamOptimizer(learning_rate=init_lr).minimize(cost)\n",
        "\n",
        "# --- start session ---\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for iter in range(num_epoch):\n",
        "        \n",
        "        # train\n",
        "        for current_batch_index in range(0,len(train_images),batch_size):\n",
        "            current_batch = train_images[current_batch_index:current_batch_index+batch_size,:,:,:]\n",
        "            current_label = train_labels[current_batch_index:current_batch_index+batch_size,:,:,:]\n",
        "            sess_results = sess.run([cost,auto_train],feed_dict={x:current_batch,y:current_label})\n",
        "            print(' Iter: ', iter, \" Cost:  %.32f\"% sess_results[0])\n",
        "        print('\\n-----------------------')\n",
        "        train_images,train_labels = shuffle(train_images,train_labels)\n",
        "\n",
        "        if iter % 2 == 0:\n",
        "            test_example =   train_images[:2,:,:,:]\n",
        "            test_example_gt = train_labels[:2,:,:,:]\n",
        "            sess_results = sess.run([layer10],feed_dict={x:test_example})\n",
        "\n",
        "            sess_results = sess_results[0][0,:,:,:]\n",
        "            test_example = test_example[0,:,:,:]\n",
        "            test_example_gt = test_example_gt[0,:,:,:]\n",
        "\n",
        "            plt.figure()\n",
        "            plt.imshow(np.squeeze(test_example),cmap='gray')\n",
        "            plt.axis('off')\n",
        "            plt.title('Original Image')\n",
        "            plt.savefig('train_change/'+str(iter)+\"a_Original_Image.png\")\n",
        "\n",
        "            plt.figure()\n",
        "            plt.imshow(np.squeeze(test_example_gt),cmap='gray')\n",
        "            plt.axis('off')\n",
        "            plt.title('Ground Truth Mask')\n",
        "            plt.savefig('train_change/'+str(iter)+\"b_Original_Mask.png\")\n",
        "\n",
        "            plt.figure()\n",
        "            plt.imshow(np.squeeze(sess_results),cmap='gray')\n",
        "            plt.axis('off')\n",
        "            plt.title('Generated Mask')\n",
        "            plt.savefig('train_change/'+str(iter)+\"c_Generated_Mask.png\")\n",
        "\n",
        "            plt.figure()\n",
        "            plt.imshow(np.multiply(np.squeeze(test_example),np.squeeze(test_example_gt)),cmap='gray')\n",
        "            plt.axis('off')\n",
        "            plt.title(\"Ground Truth Overlay\")\n",
        "            plt.savefig('train_change/'+str(iter)+\"d_Original_Image_Overlay.png\")\n",
        "\n",
        "            plt.figure()\n",
        "            plt.axis('off')\n",
        "            plt.imshow(np.multiply(np.squeeze(test_example),np.squeeze(sess_results)),cmap='gray')\n",
        "            plt.title(\"Generated Overlay\")\n",
        "            plt.savefig('train_change/'+str(iter)+\"e_Generated_Image_Overlay.png\")\n",
        "\n",
        "            plt.close('all')\n",
        "\n",
        "\n",
        "    for data_index in range(0,len(train_images),batch_size):\n",
        "        current_batch = train_images[current_batch_index:current_batch_index+batch_size,:,:,:]\n",
        "        current_label = train_labels[current_batch_index:current_batch_index+batch_size,:,:,:]\n",
        "        sess_results = sess.run(layer10,feed_dict={x:current_batch})\n",
        "\n",
        "        plt.figure()\n",
        "        plt.imshow(np.squeeze(current_batch[0,:,:,:]),cmap='gray')\n",
        "        plt.axis('off')\n",
        "        plt.title(str(data_index)+\"a_Original Image\")\n",
        "        plt.savefig('gif/'+str(data_index)+\"a_Original_Image.png\")\n",
        "\n",
        "        plt.figure()\n",
        "        plt.imshow(np.squeeze(current_label[0,:,:,:]),cmap='gray')\n",
        "        plt.axis('off')\n",
        "        plt.title(str(data_index)+\"b_Original Mask\")\n",
        "        plt.savefig('gif/'+str(data_index)+\"b_Original_Mask.png\")\n",
        "        \n",
        "        plt.figure()\n",
        "        plt.imshow(np.squeeze(sess_results[0,:,:,:]),cmap='gray')\n",
        "        plt.axis('off')\n",
        "        plt.title(str(data_index)+\"c_Generated Mask\")\n",
        "        plt.savefig('gif/'+str(data_index)+\"c_Generated_Mask.png\")\n",
        "\n",
        "        plt.figure()\n",
        "        plt.imshow(np.multiply(np.squeeze(current_batch[0,:,:,:]),np.squeeze(current_label[0,:,:,:])),cmap='gray')\n",
        "        plt.axis('off')\n",
        "        plt.title(str(data_index)+\"d_Original Image Overlay\")\n",
        "        plt.savefig('gif/'+str(data_index)+\"d_Original_Image_Overlay.png\")\n",
        "       \n",
        "        plt.figure()\n",
        "        plt.imshow(np.multiply(np.squeeze(current_batch[0,:,:,:]),np.squeeze(sess_results[0,:,:,:])),cmap='gray')\n",
        "        plt.axis('off')\n",
        "        plt.title(str(data_index)+\"e_Generated Image Overlay\")\n",
        "        plt.savefig('gif/'+str(data_index)+\"e_Generated_Image_Overlay.png\")\n",
        "\n",
        "        plt.close('all')\n",
        "\n",
        "\n",
        "# -- end code --"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:63: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0.\n",
            "Use ``matplotlib.pyplot.imread`` instead.\n",
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:63: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
            "Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n",
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:64: DeprecationWarning: `imread` is deprecated!\n",
            "`imread` is deprecated in SciPy 1.0.0.\n",
            "Use ``matplotlib.pyplot.imread`` instead.\n",
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:64: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
            "Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-9e08614e8303>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mtrain_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'F'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_gt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'F'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 128 is out of bounds for axis 0 with size 128"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPoBbBuQNjqN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "outputId": "21b6b551-0ec0-4c48-dd46-0ef43038a8ed"
      },
      "source": [
        "!pip install tensorflow==1.4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/72/a420e22dc93416d30981e87a2318823ec09a9b18631369df0e7d9d164073/tensorflow-1.4.0-cp27-cp27mu-manylinux1_x86_64.whl (40.7MB)\n",
            "\u001b[K     |████████████████████████████████| 40.8MB 73kB/s \n",
            "\u001b[?25hCollecting tensorflow-tensorboard<0.5.0,>=0.4.0rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/cd/f3d14d441eb1c5228aaf7e12e8e94895ae73e9af50383e481610b34357bd/tensorflow_tensorboard-0.4.0-py2-none-any.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 58.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.4) (2.0.0)\n",
            "Requirement already satisfied: enum34>=1.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.4) (1.1.6)\n",
            "Requirement already satisfied: protobuf>=3.3.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.4) (3.8.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.4) (0.35.1)\n",
            "Requirement already satisfied: backports.weakref>=1.0rc1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.4) (1.0.post1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.4) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==1.4) (1.16.4)\n",
            "Collecting bleach==1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: futures>=3.1.1; python_version < \"3.2\" in /usr/local/lib/python2.7/dist-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4) (3.2.0)\n",
            "Collecting html5lib==0.9999999\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 28.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python2.7/dist-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4) (0.15.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4) (3.1.1)\n",
            "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==1.4) (1.0.2)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==1.4) (5.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf>=3.3.0->tensorflow==1.4) (44.1.1)\n",
            "Building wheels for collected packages: html5lib\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html5lib: filename=html5lib-0.9999999-cp27-none-any.whl size=107222 sha256=88784bde130f953ef4585fd398ef835edb20b74040454fe77944cb68baf88eee\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
            "Successfully built html5lib\n",
            "\u001b[31mERROR: fastai 0.7.0 has requirement torch<0.4, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: html5lib, bleach, tensorflow-tensorboard, tensorflow\n",
            "  Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Found existing installation: bleach 3.1.0\n",
            "    Uninstalling bleach-3.1.0:\n",
            "      Successfully uninstalled bleach-3.1.0\n",
            "  Found existing installation: tensorflow 2.1.0\n",
            "    Uninstalling tensorflow-2.1.0:\n",
            "      Successfully uninstalled tensorflow-2.1.0\n",
            "Successfully installed bleach-1.5.0 html5lib-0.9999999 tensorflow-1.4.0 tensorflow-tensorboard-0.4.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fJWpQ4EQNgj"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np,sys,os\n",
        "from sklearn.utils import shuffle\n",
        "from imageio import imread\n",
        "from skimage.transform import resize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(678)\n",
        "tf.random.set_seed(5678)\n",
        "\n",
        "def tf_relu(x): return tf.nn.relu(x)\n",
        "def d_tf_relu(s): return tf.cast(tf.greater(s,0),dtype=tf.float32)\n",
        "def tf_softmax(x): return tf.nn.softmax(x)\n",
        "def np_sigmoid(x): 1/(1 + np.exp(-1 *x))\n",
        "\n",
        "# --- make class ---\n",
        "class conlayer_left():\n",
        "    \n",
        "    def __init__(self,ker,in_c,out_c):\n",
        "        self.w = tf.Variable(tf.random_normal([ker,ker,in_c,out_c],stddev=0.05))\n",
        "\n",
        "    def feedforward(self,input,stride=1,dilate=1):\n",
        "        self.input  = input\n",
        "        self.layer  = tf.nn.conv2d(input,self.w,strides = [1,stride,stride,1],padding='SAME')\n",
        "        self.layerA = tf_relu(self.layer)\n",
        "        return self.layerA\n",
        "\n",
        "class conlayer_right():\n",
        "    \n",
        "    def __init__(self,ker,in_c,out_c):\n",
        "        self.w = tf.Variable(tf.random_normal([ker,ker,in_c,out_c],stddev=0.05))\n",
        "\n",
        "    def feedforward(self,input,stride=1,dilate=1,output=1):\n",
        "        self.input  = input\n",
        "\n",
        "        current_shape_size = input.shape\n",
        "\n",
        "        self.layer = tf.nn.conv2d_transpose(input,self.w,\n",
        "        output_shape=[batch_size] + [int(current_shape_size[1].value*2),int(current_shape_size[2].value*2),int(current_shape_size[3].value/2)],strides=[1,2,2,1],padding='SAME')\n",
        "        self.layerA = tf_relu(self.layer)\n",
        "        return self.layerA\n",
        "\n",
        "# --- get data ---\n",
        "data_location = \"/content/drive/My Drive/Colab Notebooks/training/images\"\n",
        "train_data = []  # create an empty list\n",
        "for dirName, subdirList, fileList in sorted(os.walk(data_location)):\n",
        "    for filename in fileList:\n",
        "        if \".tif\" in filename.lower():  # check whether the file's DICOM\n",
        "            train_data.append(os.path.join(dirName,filename))\n",
        "\n",
        "data_location = \"/content/drive/My Drive/Colab Notebooks/training/mask\"\n",
        "train_data_gt = []  # create an empty list\n",
        "for dirName, subdirList, fileList in sorted(os.walk(data_location)):\n",
        "    for filename in fileList:\n",
        "        if \".tif\" in filename.lower():  # check whether the file's DICOM\n",
        "            train_data_gt.append(os.path.join(dirName,filename))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}