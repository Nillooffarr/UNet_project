{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ymTW-8tcKo3petnbluMt3AYKrvsTtt-U",
      "authorship_tag": "ABX9TyNRg4e10/FKLeMwN7v4En+F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nillooffarr/UNet_project/blob/Version1/FinaalUNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image\n",
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Input, Conv2DTranspose, Concatenate, BatchNormalization, UpSampling2D\n",
        "from keras.layers import  Dropout, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from keras import backend as K\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "import random\n",
        "import cv2\n",
        "from random import shuffle\n",
        "from skimage.io import imsave\n",
        "from skimage import io\n",
        "#we will undersample our training 2D images later (for memory and speed)\n",
        "image_rows = int(512)\n",
        "image_cols = int(512) \n",
        "all_files = os.listdir('drive/My Drive/MAT/images')\n",
        "split = int(0.95 * len(all_files))\n",
        "train_files = all_files[0:split]\n",
        "test_files  = all_files[split:]\n",
        "\n",
        "def create_train_data():\n",
        "    print('-'*30)\n",
        "    print('Creating training data...')\n",
        "    print('-'*30)\n",
        "    imgs_train = [] \n",
        "    masks_train = []  \n",
        "    for f in train_files:\n",
        "        mask = Image.open(f'drive/My Drive/MAT/masks/{f}')\n",
        "        raw = Image.open(f'drive/My Drive/MAT/images/{f}')\n",
        "        masks_train.append(mask)\n",
        "        imgs_train.append(raw)\n",
        "    \n",
        "    imgs = np.ndarray(\n",
        "            (len(imgs_train), image_rows, image_cols), dtype=np.uint8\n",
        "            )\n",
        "    imgs_mask = np.ndarray(\n",
        "           (len(masks_train), image_rows, image_cols), dtype=np.uint8\n",
        "            )\n",
        "    \n",
        "    for index, img in enumerate(imgs_train):\n",
        "        imgs[index, :, :] = img\n",
        "        \n",
        "    for index, img in enumerate(masks_train):\n",
        "        imgs_mask[index, :, :] = img\n",
        "\n",
        "    np.save('imgs_train.npy', imgs)\n",
        "    np.save('masks_train.npy', imgs_mask)\n",
        "    print('Saving to .npy files done.')\n",
        "\n",
        "\n",
        "def load_train_data():\n",
        "    imgs_train = np.load('imgs_train.npy')\n",
        "    masks_train = np.load('masks_train.npy')\n",
        "    return imgs_train, masks_train\n",
        "\n",
        "\n",
        "def create_test_data():\n",
        "    imgs_test = []\n",
        "    masks_test = []\n",
        "    print('-'*30)\n",
        "    print('Creating test data...')\n",
        "    print('-'*30)\n",
        "    Image_new = Image.open('drive/My Drive/MAT/images/Y100.png') #to test on an image outside dataset\n",
        "    for f in test_files:\n",
        "        mask = Image.open(f'drive/My Drive/MAT/masks/{f[:-4]}.png')\n",
        "        raw = Image.open(f'drive/My Drive/MAT/images/{f}')\n",
        "        masks_test.append(mask)\n",
        "        imgs_test.append(raw)               \n",
        "    imgst = np.ndarray(\n",
        "            (len(imgs_test), image_rows, image_cols), dtype=np.uint8\n",
        "            )\n",
        "    imgs_maskt = np.ndarray(\n",
        "            (len(masks_test), image_rows, image_cols), dtype=np.uint8\n",
        "            )\n",
        "    imstnn = np.ndarray(( image_rows, image_cols,3), dtype=np.uint8)\n",
        "    imstnew = np.ndarray(( 1,image_rows, image_cols), dtype=np.uint8)\n",
        "    imstnn [:,:,:] = Image_new\n",
        "    imstnew[0,:,:] = imstnn[:,:,0]       \n",
        "    for index, img in enumerate(imgs_test):\n",
        "        imgst[index, :, :] = img\n",
        "        \n",
        "    for index, img in enumerate(masks_test):\n",
        "        imgs_maskt[index, :, :] = img\n",
        "    np.save('imgs_test.npy', imgst)\n",
        "    np.save('masks_test.npy', imgs_maskt)\n",
        "    np.save('new_test.npy',imstnew)\n",
        "    print('Saving to .npy files done.')\n",
        "    \n",
        "\n",
        "def load_test_data():\n",
        "    imgs_test = np.load('imgs_test.npy')\n",
        "    masks_test = np.load('masks_test.npy')\n",
        "    new_test = np.load('new_test.npy')\n",
        "    return imgs_test, masks_test, new_test\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    create_train_data()\n",
        "    create_test_data()\n",
        "    \n",
        "    \n"
      ],
      "metadata": {
        "id": "LdW1rtzQenHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fJWpQ4EQNgj"
      },
      "source": [
        "    \n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "from skimage.transform import resize\n",
        "from skimage.io import imsave\n",
        "import numpy as np\n",
        "from skimage.segmentation import mark_boundaries\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, concatenate, Conv2D, MaxPooling2D, Conv2DTranspose\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras import backend as K\n",
        "from skimage.exposure import rescale_intensity\n",
        "from keras.callbacks import History\n",
        "from skimage import io\n",
        "import matplotlib.pyplot as plt\n",
        "K.set_image_data_format('channels_last')  # TF dimension ordering in this code\n",
        "\n",
        "img_rows = int(512/2)\n",
        "img_cols = int(512/2)\n",
        "smooth = 1.\n",
        "#We divide here the number of rows and columns by two because we undersample our data (We take one pixel over two)\n",
        "\n",
        "\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return -dice_coef(y_true, y_pred)\n",
        "#The functions return our metric and loss\n",
        "\n",
        "\n",
        "def get_unet():\n",
        "    inputs = Input((img_rows, img_cols, 1))\n",
        "    conv1 = Conv2D(32, (3, 3), activation = 'relu',padding='same')(inputs)\n",
        "    conv1 = Conv2D(32, (3, 3), activation = 'relu',padding='same')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    \n",
        "\n",
        "    conv2 = Conv2D(64, (3, 3), activation = 'relu',padding='same')(pool1)\n",
        "    conv2 = Conv2D(64, (3, 3), activation = 'relu',padding='same')(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    conv3 = Conv2D(128, (3, 3),activation = 'relu', padding='same')(pool2)\n",
        "    conv3 = Conv2D(128, (3, 3), activation = 'relu',padding='same')(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "\n",
        "    conv4 = Conv2D(256, (3, 3),activation = 'relu', padding='same')(pool3)\n",
        "    conv4 = Conv2D(256, (3, 3),  padding='same')(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "\n",
        "    conv5 = Conv2D(512, (3, 3), activation = 'relu', padding='same')(pool4)\n",
        "    conv5 = Conv2D(512, (3, 3), activation = 'relu', padding='same')(conv5)\n",
        "\n",
        "    up6 = concatenate([Conv2DTranspose(256, (2, 2),  padding='same')(conv5), conv4], axis=3)\n",
        "    conv6 = Conv2D(256, (3, 3),activation = 'relu', padding='same')(up6)\n",
        "    conv6 = Conv2D(256, (3, 3), activation = 'relu',padding='same')(conv6)\n",
        "\n",
        "    up7 = concatenate([Conv2DTranspose(128, (2, 2), padding='same')(conv6), conv3], axis=3)\n",
        "    conv7 = Conv2D(128, (3, 3), activation = 'relu',padding='same')(up7)\n",
        "    conv7 = Conv2D(128, (3, 3), padding='same')(conv7)\n",
        "\n",
        "    up8 = concatenate([Conv2DTranspose(64, (2, 2),  padding='same')(conv7), conv2], axis=3)\n",
        "    conv8 = Conv2D(64, (3, 3), activation = 'relu',padding='same')(up8)\n",
        "    conv8 = Conv2D(64, (3, 3), activation = 'relu',padding='same')(conv8)\n",
        "\n",
        "    up9 = concatenate([Conv2DTranspose(32, (2, 2),  padding='same')(conv8), conv1], axis=3)\n",
        "    conv9 = Conv2D(32, (3, 3), activation = 'relu',padding='same')(up9)\n",
        "    conv9 = Conv2D(32, (3, 3), activation = 'relu',padding='same')(conv9)\n",
        "\n",
        "    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n",
        "    #conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv8) to check what effect the first and layer have\n",
        "    model = Model(inputs=[inputs], outputs=[conv10])\n",
        "\n",
        "    model.compile(optimizer=Adam(lr=1e-5), loss=dice_coef_loss, metrics=[dice_coef])\n",
        "\n",
        "    return model\n",
        "#The different layers in our neural network model (including convolutions, maxpooling and upsampling)\n",
        "\n",
        "\n",
        "def preprocess(imgs):\n",
        "    imgs_p = np.ndarray((imgs.shape[0], img_rows, img_cols), dtype=np.uint8)\n",
        "    for i in range(imgs.shape[0]):\n",
        "        imgs_p[i] = resize(imgs[i], (img_cols, img_rows), preserve_range=True)\n",
        "    imgs_p = imgs_p[..., np.newaxis]\n",
        "    return imgs_p\n",
        "#We adapt here our dataset samples dimension so that we can feed it to our network\n",
        "\n",
        "def train_and_predict():\n",
        "    print('-'*30)\n",
        "    print('Loading and preprocessing train data...')\n",
        "    print('-'*30)\n",
        "    imgs_train, imgs_mask_train = load_train_data()\n",
        "\n",
        "    imgs_train = preprocess(imgs_train)\n",
        "    imgs_mask_train = preprocess(imgs_mask_train)\n",
        "\n",
        "    imgs_train = imgs_train.astype('float32')\n",
        "    mean = np.mean(imgs_train)  # mean for data centering\n",
        "    std = np.std(imgs_train)  # std for data normalization\n",
        "\n",
        "    imgs_train -= mean\n",
        "    imgs_train /= std\n",
        "    #Normalization of the train set\n",
        "\n",
        "    imgs_mask_train = imgs_mask_train.astype('float32')\n",
        "\n",
        "    print('-'*30)\n",
        "    print('Creating and compiling model...')\n",
        "    print('-'*30)\n",
        "    model = get_unet()\n",
        "    model_checkpoint = ModelCheckpoint('weights.h5', monitor='val_loss', save_best_only=True)\n",
        "    #Saving the weights and the loss of the best predictions we obtained\n",
        "\n",
        "    print('-'*30)\n",
        "    print('Fitting model...')\n",
        "    print('-'*30)\n",
        "    history=model.fit(imgs_train, imgs_mask_train, batch_size=6, epochs=100, verbose=1, shuffle=True,\n",
        "              validation_split=0.2,\n",
        "              callbacks=[model_checkpoint])\n",
        "\n",
        "    print('-'*30)\n",
        "    print('Loading and preprocessing test data...')\n",
        "    print('-'*30)\n",
        "    imgs_test, imgs_maskt, new_test = load_test_data()\n",
        "    imgs_test = preprocess(imgs_test)\n",
        "    imgs_test_new = preprocess(new_test)\n",
        "\n",
        "    imgs_test = imgs_test.astype('float32')\n",
        "    mean = np.mean(imgs_test)\n",
        "    std = np.std(imgs_test)\n",
        "    imgs_test -= mean\n",
        "    imgs_test /= std\n",
        "    #Normalization of the test set\n",
        "    imgs_test_new = imgs_test_new.astype('float32')\n",
        "    mean = np.mean(imgs_test_new)\n",
        "    std = np.std(imgs_test_new)\n",
        "    imgs_test_new -= mean\n",
        "    imgs_test_new /= std\n",
        "\n",
        "    print('-'*30)\n",
        "    print('Loading saved weights...')\n",
        "    print('-'*30)\n",
        "    model.load_weights('weights.h5')\n",
        "\n",
        "    print('-'*30)\n",
        "    print('Predicting masks on test data...')\n",
        "    print('-'*30)\n",
        "    imgs_mask_test = model.predict(imgs_test, verbose=1)\n",
        "    imgs_mask_new = model.predict(imgs_test_new, verbose=1)\n",
        "    np.save('imgs_mask_test.npy', imgs_mask_test)\n",
        "    np.save('imgs_mask_new.npy', imgs_mask_new)\n",
        "    print('-' * 30)\n",
        "    print('Saving predicted masks to files...')\n",
        "    print('-' * 30)\n",
        "\n",
        "    plt.plot(history.history['dice_coef'])\n",
        "    plt.plot(history.history['val_dice_coef'])\n",
        "    plt.title('Model dice coeff')\n",
        "    plt.ylabel('Dice coeff')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "    plt.show()\n",
        "    #plotting our dice coeff results in function of the number of epochs\n",
        "\n",
        "    pred_dir = 'preds'\n",
        "    seg_dir = 'seg'\n",
        "    if not os.path.exists(seg_dir):\n",
        "        os.mkdir(seg_dir)\n",
        "    if not os.path.exists(pred_dir):\n",
        "        os.mkdir(pred_dir)\n",
        "\n",
        "    mskn = imgs_mask_new[0][:,:,0]\n",
        "    a=rescale_intensity(imgs_test_new[0][:,:,0],out_range=(-1,1))\n",
        "    b=(mskn).astype('uint8')\n",
        "    mskn[mskn >= 0.5] = 1 \n",
        "    mskn[mskn < 0.5] = 0 \n",
        "    io.imsave(os.path.join(pred_dir, 'new_pred.png'),mark_boundaries(a,b))\n",
        "    combined = np.concatenate([a, mskn, a*mskn], axis = 1)\n",
        "    io.imsave(os.path.join(seg_dir,  'new_seg.png'),combined)\n",
        "    for k in range(len(imgs_mask_test)):\n",
        "        msk = imgs_mask_test[k][:,:,0]\n",
        "        a=rescale_intensity(imgs_test[k][:,:,0],out_range=(-1,1))\n",
        "        b=(msk).astype('uint8')\n",
        "        msk[msk >= 0.5] = 1 \n",
        "        msk[msk < 0.5] = 0 \n",
        "\n",
        "        combined = np.concatenate([a, msk, a*msk], axis = 1)\n",
        "        io.imsave(os.path.join(seg_dir, str(k) + '_seg.png'),combined)\n",
        "        io.imsave(os.path.join(pred_dir, str(k) + '_pred.png'),mark_boundaries(a,b))\n",
        "\n",
        "\n",
        "    #Saving our predictions in the directory 'preds' and 'Seg'\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}